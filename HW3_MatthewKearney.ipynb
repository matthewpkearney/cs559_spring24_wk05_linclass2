{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classification II - Naive Bayes Classification Homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#maybe make dataframe and get data description.\n",
    "X, y, centers = datasets.make_blobs(n_samples = 400, n_features = 5, \n",
    "            centers=4, cluster_std = 2, random_state=100, return_centers=True)\n",
    "\n",
    "DF_y= pd.DataFrame(X)\n",
    "DF_y['y']= y\n",
    "# DF[4] = pd.DataFrame.from_records(y)\n",
    "# DF_y.describe()\n",
    "# print(DF[0])\n",
    "\n",
    "N, D = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Compute the prior probability of each class, p(Ck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the problem statement specifies the amount of observations in each target class, we can infer that all prior probabilities are equal at 1/4. We will want to do the computation to ensure this fact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior p(c0): 0.25\n",
      "prior p(c1): 0.25\n",
      "prior p(c2): 0.25\n",
      "prior p(c3): 0.25\n"
     ]
    }
   ],
   "source": [
    "c0= 0\n",
    "c1= 0\n",
    "c2= 0\n",
    "c3= 0\n",
    "\n",
    "for i in y: \n",
    "    if i == 0:\n",
    "        c0=c0+1\n",
    "    if i == 1:\n",
    "        c1=c1+1\n",
    "    if i == 2: \n",
    "        c2=c2+1\n",
    "    if i == 3:\n",
    "        c3=c3+1\n",
    "        \n",
    "\n",
    "ck = [0, 1, 2, 3]\n",
    "a = [c0,c1,c2,c3]\n",
    "ct = 0\n",
    "\n",
    "priors = []\n",
    "for i in a:\n",
    "    priors.append(i/N)\n",
    "\n",
    "for i in priors: \n",
    "    print(\"prior p(c\" + str(ck[ct]) + \"): \" + str(i))\n",
    "    ct = ct +1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Compute the likelihood p(X|Ck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As naive bayes assumes that our classes are independent although they may not be, we can simplify the likelihood \n",
    "P(X0, X1, X2, X3 | Y) to just i=1 ∏ D p(Xi | Y). For this, we must find the distribution (pdf~ probability density function) p(feature(i) | Y) accounting for target correlation using μk and σk^(2) where k denotes the number of classes and the belonging class. In other words we must find the distribution for each feature that maps to each target, and we should expect 4 features x 4 classes = 16 values in the final likelihood calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [0,1,2,3,4]\n",
    "means = DF_y.groupby('y')[feats].mean()\n",
    "variances = DF_y.groupby('y')[feats].var()\n",
    "\n",
    "means = means.to_numpy()\n",
    "variances = variances.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, it will be crucial to draw normal distributions coming from each feature given each class (target value). With the means and variances computed, we use the probability density function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf(mean, variance, xi):\n",
    "    t1= 1/(variance*np.sqrt(2*np.pi))\n",
    "    return t1*np.exp(-((xi - mean) ** 2) / (2 * variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-45.6985698 , -84.05970182, -55.50213622, -13.08991735],\n",
       "       [-43.67078578, -68.5409193 , -18.30734434, -71.81973225],\n",
       "       [-12.54998639, -67.10163115, -54.08446763, -45.33746018],\n",
       "       ...,\n",
       "       [-46.42621433, -69.91300807, -47.59980544, -14.30066169],\n",
       "       [-41.37699859, -66.05509473, -12.0967166 , -50.50308082],\n",
       "       [-38.15476588, -62.83221841, -37.61153955, -15.48573157]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood= np.zeros((N, len(ck))) ## N X len(y) likelihood matrix\n",
    "\n",
    "for i in range(len(ck)): ## i ~ classes = 0 -> 3\n",
    "    likelihoods = np.zeros(N)\n",
    "    for x in range(D): ## x ~ features = 0 -> 4\n",
    "        likelihoods+=np.log(pdf(means[i, x], variances[i, x], X[:, x]))\n",
    "    likelihood[:, i] = likelihoods\n",
    "\n",
    "likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Compute the posterior probability of each point p(Ck|X). Assign the class ID to each point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We understand that the posterior probability is proportional to the likelihood times the prior probabilities. By using np.argmax, we assign classes or labels to each observation in predicting its designated class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 0, 3, 3, 1, 3, 1, 2, 2, 0, 2, 2, 3, 0, 3, 2, 3, 1, 2, 2, 1,\n",
       "       1, 3, 1, 3, 1, 0, 2, 0, 2, 3, 0, 0, 3, 0, 1, 1, 1, 2, 1, 3, 2, 0,\n",
       "       0, 3, 3, 2, 3, 1, 3, 1, 2, 2, 3, 1, 0, 1, 3, 2, 2, 0, 3, 1, 1, 0,\n",
       "       3, 2, 1, 0, 1, 2, 1, 2, 0, 0, 1, 1, 2, 0, 0, 1, 1, 0, 1, 0, 0, 3,\n",
       "       2, 1, 1, 3, 3, 0, 2, 1, 3, 2, 3, 1, 2, 2, 1, 1, 1, 1, 2, 3, 2, 3,\n",
       "       1, 3, 0, 3, 2, 2, 1, 1, 1, 3, 1, 3, 0, 3, 0, 2, 1, 3, 3, 2, 3, 2,\n",
       "       3, 0, 2, 2, 2, 3, 1, 3, 2, 0, 0, 0, 0, 2, 3, 1, 3, 0, 0, 1, 3, 1,\n",
       "       2, 0, 0, 3, 0, 3, 1, 0, 0, 2, 0, 3, 1, 2, 2, 3, 0, 0, 3, 1, 1, 0,\n",
       "       3, 0, 0, 0, 1, 0, 1, 0, 1, 0, 3, 2, 1, 2, 2, 0, 1, 3, 2, 3, 3, 0,\n",
       "       3, 2, 0, 0, 1, 3, 0, 3, 3, 2, 1, 3, 0, 1, 2, 2, 3, 1, 2, 1, 2, 3,\n",
       "       1, 0, 1, 3, 1, 0, 0, 1, 0, 0, 3, 0, 1, 0, 3, 1, 3, 3, 0, 0, 1, 3,\n",
       "       0, 0, 2, 1, 2, 2, 0, 3, 0, 0, 1, 3, 3, 2, 0, 0, 1, 2, 2, 1, 3, 0,\n",
       "       2, 1, 3, 3, 2, 1, 0, 2, 0, 0, 3, 3, 2, 0, 3, 2, 2, 3, 2, 2, 1, 0,\n",
       "       3, 0, 1, 2, 0, 3, 1, 1, 2, 2, 0, 0, 2, 3, 1, 0, 2, 0, 0, 1, 0, 1,\n",
       "       1, 3, 2, 0, 1, 3, 3, 1, 3, 2, 2, 3, 2, 1, 1, 1, 1, 2, 2, 3, 2, 0,\n",
       "       0, 2, 2, 0, 1, 1, 0, 1, 2, 1, 3, 1, 2, 0, 0, 1, 0, 3, 3, 2, 2, 2,\n",
       "       2, 0, 3, 2, 3, 2, 1, 3, 2, 1, 1, 2, 2, 2, 3, 2, 2, 0, 2, 0, 2, 0,\n",
       "       1, 2, 0, 3, 1, 3, 2, 1, 3, 3, 3, 3, 0, 3, 1, 0, 1, 0, 0, 2, 3, 1,\n",
       "       1, 3, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posterior = likelihood*priors\n",
    "# posterior\n",
    "\n",
    "predicted_classes = np.argmax(posterior, axis=1) #attach labels...\n",
    "predicted_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Construct the confusion matrix to show the classification rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix = np.zeros((len(ck), len(ck)), dtype=int)\n",
    "for t, p in zip(y, predicted_classes):\n",
    "    confusion_matrix[t, p] += 1\n",
    "\n",
    "confusion_matrix/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can be certain that all points are accurately classified, seeing as the classification rate is 100% for all classes, meaning the model accurately classified the 100 points that each class began with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Classify the target using sklearn.native bayes.GaussianNB. Report the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = GaussianNB()\n",
    "model.fit(X, y)\n",
    "y_hat = model.predict(X)\n",
    "accuracy = accuracy_score(y, y_hat)\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both my model and the model implemented by SciKit learn have identical performance at 100% accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
